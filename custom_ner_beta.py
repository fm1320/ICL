# -*- coding: utf-8 -*-
"""Custom NER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xqgZgg1WdotcvU8fs6j3OfGXd86_1Bj8
"""

#Importing drive contents 
from google.colab import drive
drive.mount('/content/drive')
#Import assay list
assay=[]
df = pd.read_csv("/content/drive/My Drive/Imperial college London/alla.csv",dtype= str) # READ CSV AS STRING !!!
assay = df.values.tolist()
assay = list(map(''.join, assay)) # convert list of lists to list of strings

# Traning additional entity type using Spacy
# Because such a traning would need a lot of formating data, it was done using 
# a only one example, the NMR assay. Bigger and formated data set would be needed for a better model
#!/usr/bin/env python
# coding: utf8
"""Example of training an additional entity type

This script shows how to add a new entity type to an existing pretrained NER
model. To keep the example short and simple, only four sentences are provided
as examples. In practice, you'll need many more — a few hundred would be a
good start. You will also likely need to mix in examples of other entity
types, which might be obtained by running the entity recognizer over unlabelled
sentences, and adding their annotations to the training set.

The actual training is performed by looping over the examples, and calling
`nlp.entity.update()`. The `update()` method steps through the words of the
input. At each word, it makes a prediction. It then consults the annotations
provided on the GoldParse instance, to see whether it was right. If it was
wrong, it adjusts its weights so that the correct action will score higher
next time.

After training your model, you can save it to a directory. We recommend
wrapping models as Python packages, for ease of deployment.

For more details, see the documentation:
* Training: https://spacy.io/usage/training
* NER: https://spacy.io/usage/linguistic-features#named-entities

Compatible with: spaCy v2.1.0+
Last tested with: v2.2.4
"""
from __future__ import unicode_literals, print_function

import plac
import random
import warnings
from pathlib import Path
import spacy
from spacy.util import minibatch, compounding


# new entity label
LABEL = "ASSAY"

# training data
# Note: If you're using an existing model, make sure to mix in examples of
# other entity types that spaCy correctly recognized before. Otherwise, your
# model might learn the new type, but "forget" what it previously knew.
# https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting

# Train_data= [ ("tekst1",{"entities":[(start_char,end_char, LABEL)]},) , ("tekst2",{"entities":[(start_char,end_char, LABEL)]},) , ... ]
# In Word, do a search/replace, replacing all spaces with paragraph breaks (^p). Select all the text and then copy/paste it into Excel.
print(lista)
TRAIN_DATA = [('I really like NMR spectroscopy.', {'entities': [(14, 17, 'ASSAY'), (18, 30, 'ASSAY')]}), ('Electrospray ionization is my favorite form of ionizing things because it can be very versatile, and as such Electrospray ionization is widely used.', {'entities': [(0, 23, 'ASSAY'), (109, 132, 'ASSAY')]}), ('The word chroma comes from the greek word for color, that is why Gas chromatography is called like that!', {'entities': [(65, 83, 'ASSAY')]})]
              
              #('what is the price of polo?', {'entities': [(11, 15, LABEL),(20, 21, LABEL)]}), 
              #('what is the price of ball?', {'entities': [(21, 25, LABEL)]}), 
              #('what is the price of jegging?', {'entities': [(21, 28, LABEL)]}), ]
#{'content': 'I really like NMR spectroscopy.', 'entities': [[14, 17, 'ASSAY'], [14, 30, 'ASSAY']]}, 
#{'content': 'Electrospray ionization is my favorite form of ionizing things because it can be very versatile, and as 
#such Electrospray ionization is widely used.', 'entities': [[0, 23, 'ASSAY'], [109, 132, 'ASSAY']]}, 
#{'content': 'The word chroma comes from the greek word for color, that is why Gas 
#chromatography is called like that!', 'entities': [[65, 83, 'ASSAY']]} ]


@plac.annotations(
    model=("Model name. Defaults to blank 'en' model.", "option", "m", str),
    new_model_name=("New model name for model meta.", "option", "nm", str),
    output_dir=("Optional output directory", "option", "o", Path),
    n_iter=("Number of training iterations", "option", "n", int),
)
def main(model="en_core_web_sm", new_model_name="ASSAY", output_dir=None, n_iter=35):
    """Set up the pipeline and entity recognizer, and train the new entity."""
    random.seed(0)
    if model is not None:
        nlp = spacy.load(model)  # load existing spaCy model
        print("Loaded model '%s'" % model)
    else:
        nlp = spacy.blank("en")  # create blank Language class
        print("Created blank 'en' model")
    # Add entity recognizer to model if it's not in the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if "ner" not in nlp.pipe_names:
        ner = nlp.create_pipe("ner")
        nlp.add_pipe(ner)
    # otherwise, get it, so we can add labels to it
    else:
        ner = nlp.get_pipe("ner")

    ner.add_label(LABEL)  # add new entity label to entity recognizer
    # Adding extraneous labels shouldn't mess anything up
    
    ner.add_label("ASSAY")
    if model is None:
        optimizer = nlp.begin_training()
    else:
        optimizer = nlp.resume_training()
    move_names = list(ner.move_names)
    
    # get names of other pipes to disable them during training
    pipe_exceptions = ["ner", "trf_wordpiecer", "trf_tok2vec"]
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]
    # only train NER
    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():
        # show warnings for misaligned entity spans once
        warnings.filterwarnings("once", category=UserWarning, module='spacy')

        sizes = compounding(1.0, 4.0, 1.001)
        # batch up the examples using spaCy's minibatch
        for itn in range(n_iter):
            random.shuffle(TRAIN_DATA)
            batches = minibatch(TRAIN_DATA, size=sizes)
            losses = {}
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)
            print("Losses", losses)

    # test the trained model
    test_text = "High quality antique sheets of paper have been characterized by 1H NMR relaxations and 13C CP MAS spectra."
    doc = nlp(test_text)
    print("Entities in '%s'" % test_text)
    for ent in doc.ents:
        print(ent.label_, ent.text)

    # save model to output directory
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.meta["name"] = new_model_name  # rename model
        nlp.to_disk(output_dir)
        print("Saved model to", output_dir)

        # test the saved model
        print("Loading from", output_dir)
        nlp2 = spacy.load(output_dir)
        # Check the classes have loaded back consistently
        assert nlp2.get_pipe("ner").move_names == move_names
        doc2 = nlp2(test_text)
        for ent in doc2.ents:
            print(ent.label_, ent.text)

if __name__ == "__main__":
    x=main()

# -----------------------------------------------
# Make text into sentences the hard way but
# Computationally LESS expensive than Spacy 
# -----------------------------------------------
# 
# -*- coding: utf-8 -*-
import re
alphabets= "([A-Za-z])"
prefixes = "(Mr|St|Mrs|Ms|Dr|Prof)[.]"
suffixes = "(Inc|Ltd|Jr|Sr|Co)"
starters = "(Mr|Mrs|Ms|Dr|He\s|She\s|It\s|They\s|Their\s|Our\s|We\s|But\s|However\s|That\s|This\s|Wherever)"
acronyms = "([A-Z][.][A-Z][.](?:[A-Z][.])?)"
websites = "[.](com|net|org|io|gov|me|ai|edu)"

def split_into_sentences(text):
    text = " " + text + "  "
    text = text.replace("\n"," ")
    text = re.sub(prefixes,"\\1<prd>",text)
    text = re.sub(websites,"<prd>\\1",text)
    if "Ph.D" in text: text = text.replace("Ph.D.","Ph<prd>D<prd>")
    text = re.sub("\s" + alphabets + "[.] "," \\1<prd> ",text)
    text = re.sub(acronyms+" "+starters,"\\1<stop> \\2",text)
    text = re.sub(alphabets + "[.]" + alphabets + "[.]" + alphabets + "[.]","\\1<prd>\\2<prd>\\3<prd>",text)
    text = re.sub(alphabets + "[.]" + alphabets + "[.]","\\1<prd>\\2<prd>",text)
    text = re.sub(" "+suffixes+"[.] "+starters," \\1<stop> \\2",text)
    text = re.sub(" "+suffixes+"[.]"," \\1<prd>",text)
    text = re.sub(" " + alphabets + "[.]"," \\1<prd>",text)
    if "”" in text: text = text.replace(".”","”.")
    if "\"" in text: text = text.replace(".\"","\".")
    if "!" in text: text = text.replace("!\"","\"!")
    if "?" in text: text = text.replace("?\"","\"?")
    text = text.replace(".",".<stop>")
    text = text.replace("?","?<stop>")
    text = text.replace("!","!<stop>")
    text = text.replace("<prd>",".")
    sentences = text.split("<stop>")
    sentences = sentences[:-1]
    sentences = [s.strip() for s in sentences]
    return sentences

# Easy way but computentianlly expensive
import spacy
nlp = spacy.load('en_core_web_sm')

txt = '''I really like NMR spectroscopy. Electrospray ionization is my favorite form of ionizing things because it can be very versatile, and as such Electrospray ionization is widely used. The word chroma comes from the greek word for color, that is why Gas chromatography is called like that!'''
tokens = nlp(txt)
sentences=[]
for sent in tokens.sents:
    sentences.append(sent.string.strip())

# Beta V 1.1
# IN PROGRESS
# Version incldes finding nouns and then doing the matching 
# Efficient vs non Efficient way
'''
This code reads into a list all the assay methods from a csv file.
A function takes a text as an input an a python list of assays. 
The function performs matching of all sentences that contain a word which 
is in the list. This is done using regular expression, the word is found 
and then the expression looks for puntcuation signs left from the word and 
right from the word so that it can extract the sentence which contains the word.

The user can choose the given assay when running the function, 
or if he doesn't input a parameter it will go through the list of all assays in the file.

'''
from google.colab import drive
drive.mount('/content/drive')

import re #time complexity issues of finite state machines ?
import pandas as pd
import spacy
import numpy as np

assay=[]
df = pd.read_csv("/content/drive/My Drive/Imperial college London/alla.csv",dtype= str) # READ CSV AS STRING !!!
assay = df.values.tolist()
assay = list(map(''.join, assay)) # convert list of lists to list of strings 

def finder(text,user_assay):    
    pattern1=r'[^.?!]*(?<=[.?\s!])%s(?=[\s.?!])[^.?!]*[.?!]' #extracts full sentences that contain a word 
    pattern2=r'\b%s\b' #extracts a given word 
    index=[]
    for i in range(len(assay)):
      tmp=re.findall(pattern2 %assay[i],text)  
      if (len(tmp)>0):
       index.append(i)
    res_list = [assay[j] for j in index]
    print("The assays mentioned are: \n ", res_list, type(res_list))
    return res_list
    '''
      #return 0
     #if the user chooses a custom assay not from the list
    else:  
     out2=re.findall(pattern % user_assay , text, re.MULTILINE )
     if (len(out2) == 0):
      print("No matching assays")
     else:
      output.append(out2) 
      print("At least one match was found :")
      print('\n'.join(out2))
'''

# Run the program outside of the function  
# Expected assay output: NMR spectroscopy, liquid chromatography, (LC)-mass spectrometry, gas chromatography-mass spectrometry, 1H NMR spectroscopy, NMR spcetrospcy  
text = ('NMR Metabolomics is used to determine the metabolic profile of biological samples, identify specific biomarkers,' 
       'and explore possible metabolic pathways. It has been used during drug development [1], '
       'and in clinical disease research [2, 3], pathology [4], toxicology [5] and nutrition studies [6].' 
       'Metabolomics mainly utilizes NMR spectroscopy [7], liquid chromatography (LC)–mass spectrometry [8] and ' 
       'gas chromatography–mass spectrometry [9] to analyze and evaluate biological specimens. '
       'Each analytical technique has its own advantages and shortcomings. none of them can be used individually' 
       'to systematically and accurately identify metabolites in complex biological matrices. '
       'Since accurate metabolite identification directly determines the usefulness of the metabolomic analysis,' 
       'metabolite identification has gained increased attention from the metabolomics research community. '
       '1H NMR spectroscopy is often used for metabolomics research. As all 1H nucleuses have the same sensitivity,' 
       'the reproducibility of NMR spectroscopy is typically high.')   


#x=finder(text,"") # You should write "" instead of None for the input of the function
nlp=spacy.load("en_core_web_sm")
doc=nlp(text)
nouns=list(doc.noun_chunks)
nouns=''.join(str(nouns)) # putting the nouns in a string because my function takes in a string
nouns=nouns.replace(',','.') # I join them with dots because I tried to fix the regex to recognize commas but it didn't work (argument agains regex)
in_txt=sentences # copy the list sentences to another list in_txt - sentences keeps intackt as a list of the sentences
in_txt=''.join(str(in_txt)) # convert in_txt to string by joining it from self, I need it all as one string for the finder function 
# I keep sentences as a list but create a string which is my input text becuase I need a string for it to be readable with the regex, but a list of the loops
x=finder(in_txt,"") # You should write "" instead of None for the input of the function 
# x is the assays found in the text , 
# in_txt is only needed to find the mentioned assays 
# Find span of all texts
#Let's see if it works with overlapping matches 
# Тука еден for ќе стаам со римув : Ако се исти и ако ендс - стартс < ендс+1 - стартс+1
# How to make non overlapping matches 
print(len(x))

#Saving to a JSON file that is compatible with Spacy NER
import json 
label="ASSAY" # za ako nema entity isto bitno e !!, i ke treba starts i ends za sekoja recenica, ako se prazni is okay
# ako e prva staj zagrada ako ne, nishto
starts=[] 
ends=[]
# Try to save in dictironary, each sentence has a seperate dictionary
# Za ova so dolnava crta da naucam bolje kako se praj sho kako

dicts = [{} for _ in range(len(sentences))] # create a list of dictionaries that has the same number of diconaries as sentences 
for k in range (len(sentences)):
  d={"content":None,"entities":[]} # Specify the dictionary format 
  dicts[k].update(d)              # Write an empty list of dictionaries 
# Functions to append multiple values to a key in dictionary

def add_content_in_dict(sample_dict, key, list_of_values):
    """Append multiple values to a key in the given dictionary"""
    if key not in sample_dict:
        sample_dict[key] = ""
    sample_dict[key]=(list_of_values) # because it initializes, it replaces the first key value pair
    return sample_dict

def add_entites_in_dict(sample_dict, key, list_of_values):
    """Append multiple values to a key in the given dictionary"""
    if key not in sample_dict:
        sample_dict[key] = []
    #if sample_dict.get(key)== []:
        #sample_dict[key]=[list_of_values]
    sample_dict[key].append(list_of_values)  # it adds to the first key value pair
    return sample_dict

# This loop finds the matches "found entities in ALL sentences" saved in the output of the finder function which is a list x[]
# It looks for the span of matches and the m
# OVERLAPPING ISSUE FIX
for k in range(len(sentences)): # cycle through sentences
  print("Matches in" , k , "sentence:")
  add_content_in_dict(dicts[k],'content',sentences[k])
  for j in range(len(x)): # look for all entites in a given sentence 
    for m in re.finditer(x[j], sentences[k]):  # cycle through all charaters in a given sentence with a given entity
        print(m.start(), m.end(), m.group()) # print the starting char, ending char , and match IN THE given sentence
        add_entites_in_dict(dicts[k],'entities',(m.start(),m.end(),label)) # updatni entities za TOJ istiot content
        #[19,30,"subject"]
        #add_entites_in_dict(dicts[k],'entities',[])
        
        starts.append(m.start())  # add to a list of all starting characters
        ends.append(m.end())      # add to a list of all ending charachters 

#starts.sort()  # sort the lists because I need the pairs of elements to find the span of the match
#ends.sort()    # I will first try with overlapping entites but I might also correct that 
#print(starts,ends)

# Try to save in dictironary, each sentence has a seperate dictionary
# Za ova so dolnava crta da naucam bolje kako se praj sho kako

# Granicniot slucaj za koa nema najdeno entities sho da stai
lista=[]

dr = [{} for _ in range(len(sentences))] 
for k in range (len(sentences)):
  d={'entities':dicts[k].get('entities')}
  dr[k].update(d) 


for i in range (len(sentences)):
  elem=(dicts[i].get('content'),dr[i])
  #print(elem)
  lista.append(elem)

# lista  =   [  (get.key, {'entities': [(),()]})   ,    (get.key, {'entities': [(),()]})   ]
#dicts[0].get('content') # ova e getkey


print(lista)
